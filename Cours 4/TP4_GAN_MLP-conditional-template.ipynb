{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP4: univariate GANs for financial time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "SPY=yfinance.Ticker(\"^GSPC\")\n",
    "mydata=SPY.history(start=\"1900-01-01\",auto_adjust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets=np.diff(np.log(mydata['Close']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=252\n",
    "\n",
    "dim_noise=????   # as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Reshape,Flatten, Dropout, Lambda, RepeatVector,Concatenate\n",
    "from tensorflow.keras.layers import Add,Multiply\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import UpSampling1D, Conv1D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def generator_model_mlp(dim_conds,dim_noise,use_bias=False):\n",
    "    input_conds = Input(shape=(dim_conds,))\n",
    "    input_eta = Input(shape=(dim_noise,))\n",
    "    \n",
    "    input_merged = Concatenate()([input_conds, input_eta])\n",
    "    \n",
    "#   define here model, an object in the functional way:    \n",
    "\n",
    "    \n",
    "    output_merged = Concatenate()(###### FILL ME ######) \n",
    "   \n",
    "    return Model([input_conds,input_eta],output_merged)    # syntax: Model(inputs,outputs)\n",
    "    \n",
    "\n",
    "def discriminator_model_mlp(T,use_bias=True,activation=\"relu\"):\n",
    "    \n",
    "#### implement here a discriminator ####\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=generator_model_mlp(T-1,dim_noise,use_bias=True)\n",
    "\n",
    "discriminator=discriminator_model_mlp(T,use_bias=True)\n",
    "\n",
    "\n",
    "INIT_LR =?==# as in the paper\n",
    "\n",
    "opt_discriminator = ????    # Check if the paper uses the same one\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=opt_discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "ganInput = [Input(shape=(T-1,)),Input(shape=(dim_noise,))]       # two types of inputs\n",
    "ganOutput = discriminator(generator(ganInput))\n",
    "gan = Model(ganInput, ganOutput)\n",
    "# compile the GAN\n",
    "opt_gan = ????\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=opt_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "num_epochs=2000\n",
    "batch_size=128\n",
    "batch_size_RMSE=5000            # number of samples to compute the RMSE\n",
    "\n",
    "num_batches_per_epoch=int(len(rets)/batch_size)\n",
    "losses=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for myepoch in range(num_epochs):\n",
    "    print(\"epoch \"+str(myepoch+1)+\"/\"+str(num_epochs))\n",
    "    for batch in range(num_batches_per_epoch//10):\n",
    "        rets_sample=[rets[range(i,i+T)] for i in np.random.choice(len(rets)-T,size=batch_size)]\n",
    "        rets_sample=np.array(rets_sample)         # list -> 2d-array\n",
    "        rets_sample=scale(rets_sample,axis=1)\n",
    "\n",
    "        conds=rets_sample[:,:-1]\n",
    "        etas=np.random.normal(size=(batch_size,dim_noise))     # generates the feature noise\n",
    "\n",
    "        preds=generator.predict(##### PLEASE FILL ME #####)\n",
    "      \n",
    "        X=np.concatenate((preds,rets_sample))\n",
    "        y=np.append(np.full(batch_size,0),np.full(batch_size,1))\n",
    "        (X,y)=shuffle(X,y)   # otherwise the discriminator learns the order\n",
    "\n",
    "        X=np.expand_dims(X, axis=2)\n",
    "        y=np.expand_dims(y,1)\n",
    "\n",
    "        discriminator_loss=discriminator.train_on_batch(X, y)    # improves discriminator classifiation abilities\n",
    "                                                                 # this temporarilly unfreezes the discr. weights\n",
    "\n",
    "            \n",
    "        rets_sample=[rets[range(i,i+T)] for i in np.random.choice(len(rets)-T,size=batch_size)]\n",
    "        rets_sample=np.array(rets_sample)         # list -> 2d-array\n",
    "        rets_sample=scale(rets_sample,axis=1)\n",
    "\n",
    "        conds=rets_sample[:,:-1]\n",
    "        etas=np.random.normal(size=(batch_size,dim_noise))     \n",
    "\n",
    "        y_fake=np.full(etas.shape[0],1)\n",
    "\n",
    "        gan_loss = gan.train_on_batch(##### PLEASE FILL ME #####,y_fake)               # trains the generator\n",
    "    \n",
    "    \n",
    "    rets_sample=[rets[range(i,i+T)] for i in np.random.choice(len(rets)-T,size=batch_size_RMSE)]\n",
    "    rets_sample=np.array(rets_sample)         # list -> 2d-array\n",
    "    rets_sample=scale(rets_sample,axis=1)\n",
    "\n",
    "    conds=rets_sample[:,:-1]\n",
    "    etas=np.random.normal(size=(batch_size_RMSE,dim_noise))     # replace the last column by noise\n",
    "\n",
    "\n",
    "    RMSE=np.sqrt(np.mean((generator.predict(##### PLEASE FILL ME #####)[:,-1]-rets_sample[:,-1])**2))\n",
    "    \n",
    "    new_losses=[gan_loss,discriminator_loss,RMSE]\n",
    "    print(new_losses)\n",
    "    losses.append(np.array(new_losses))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
